—— 突破记忆瓶颈与 RAG 背包
📖 荒岛日记：第 8 天
今早醒来，我发现昨晚剩下的半块熏肉不见了。 我问 Friday：“我们把昨晚的剩肉放哪了？”
Friday 眨了眨眼，自信地回答：“根据我的数据库，熏肉通常存放在冰箱冷冻室，建议温度 -18°C。”
我差点背过气去。这里是荒岛，哪来的冰箱？ 昨天我明明告诉过他：“肉放在岩石缝隙里，用芭蕉叶盖着。” 但这句话已经被后来我们关于“搭建厕所”的几千字对话给挤出了他的大脑。
我意识到，Friday 虽然聪明，但他是个金鱼脑。 他的“大脑皮层”（Context Window）容量是有限的。一旦新的对话涌入，旧的记忆就会像传送带末端的货物一样掉进深渊。
如果我想让他记住整座岛屿的地图、所有的草药知识，光靠他那个昂贵且有限的大脑是不够的。 我不能指望他“背诵”所有知识，我得给他一个背包（Vector Database），把记不住的东西写在书里放进去。
🛠️ 技术生存手册：把知识“压缩”进背包
为了解决这个问题，我需要在沙滩上重新设计 Friday 的记忆架构。这涉及到 AI 领域最热门的技术——RAG（Retrieval-Augmented Generation，检索增强生成）。
我在沙地上画了两个圈：
1. 脑子 (LLM Context Window):
• 特点： 贵、快、容量有限（比如 8k - 128k Tokens）。
• 现状： 塞满了当前的对话，稍微远一点的事就忘了。
2. 背包 (Vector Database / External Knowledge):
• 特点： 便宜、容量无限、存取稍微慢一点。
• 原理： 我们不能把整本书塞进 Friday 脑子里，但我们可以把书撕成一页页的片段（Chunks），存进背包。
• 关键技术——Embedding（向量化）： 为了能快速找到“熏肉”那一页，我不能一页页翻。我要把每段文字变成坐标（Vector）。
💻 开发者日志：缝制“向量背包”
我打开 HelloAgents 的源码，准备给 Friday 动个大手术。我要引入一个外挂知识库。
在《从零开始构建智能体》的教程中，这通常需要使用 ChromaDB 或 Faiss 等向量数据库工具。但在荒岛上，我先用 Python手写一个最简化的版本来演示原理。
# RAG 系统核心逻辑：给 Friday 加上外接大脑

class KnowledgeBase:
    def __init__(self):
        self.documents = [] # 这就是背包
        
    def add_document(self, text):
        # 1. 存入知识：实际应用中这里会调用 Embedding 模型把 text 变成向量
        # 这里为了演示，我们假设直接存文本
        self.documents.append(text)
        print(f"知识已存入背包: {text[:10]}...")

    def search(self, query):
        # 2. 检索知识：在背包里找和 query 最相关的片段
        # 实际应用中是用向量相似度计算 (Cosine Similarity)
        # 这里用简单的关键词模拟
        results = [doc for doc in self.documents if any(word in doc for word in query.split())]
        return results if results else None

class Agent:
    def __init__(self, name, knowledge_base):
        self.llm = LLM()
        self.kb = knowledge_base  # 装备背包
        
    def answer(self, question):
        # 第一步：先不急着回答，先翻书（Retrieve）
        related_info = self.kb.search(question)
        
        # 第二步：把找出来的知识放在 Prompt 里（Augment）
        if related_info:
            prompt = f"""
            参考资料：{related_info}
            基于上述资料，回答问题：{question}
            """
        else:
            prompt = question
            
        # 第三步：生成回答（Generate）
        return self.llm.generate(prompt)
代码写好后，我立刻开始录入数据。
“Friday，开启记录模式。” 我把他之前忘掉的所有关键信息，一条条录入了新的数据库：
• “熏肉在岩石缝隙。”
• “红色的果子不能吃。”
• “淡水源在西北角 500 米处。”
录入完毕，我再次问道：“Friday，我们要吃饭了，肉在哪？”
这一次，Friday 没有直接瞎编。他停顿了 0.5 秒（这是检索模块在通过 Embedding 向量在数据库中进行余弦相似度搜索），然后眼睛一亮： “检索到相关记录：‘熏肉在岩石缝隙，用芭蕉叶盖着’。建议立即取出食用。”
成功了！
通过 RAG，Friday 不再受限于他那个只有金鱼记忆般的 Context Window。只要背包（数据库）够大，他可以装下整个人类文明的百科全书，并在需要的时候精准地把那一页翻出来。
但我很快发现，随着背包里的东西越来越多，Friday 开始变得有点“迟钝”。有时候我只是随口问一句“你好吗”，他也要去翻半天书。而且，对话历史越来越长，我该如何让他只记住重要的事情，忘掉废话呢？
下一章，我们将探讨“上下文工程（Context Engineering）”，通过精心设计的压缩机制，防止 Friday 的大脑过载。
